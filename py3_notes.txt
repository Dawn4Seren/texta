Just some notes to keep track of certain things

# header
+ = WORKS
- = DOESN'T
+- = DIDN'T BUT NOW DOES
? = UNCLEAR
=== === = category within a header
== = note or something to do
+= = now implomented succesfully/note tested

### WHAT WORKS/DOESNT:

####################### HOME + ########################################
    + Change Password



###################### TERMINOLOGY MANAGEMENT + #################################
    # term overview
        = better ux
    # SEARCHER +
        === current search === +
            + search
            + regular highlighting, but not on the txt_5000 file as mentioned below, might be from whitespace
            + translit highlighting
            +- short search
            + expand search
            + save search
            +- bring up saved searches
            +- concept/lexicon suggestions

        === cluster search === +
            +- cluster search clustering method: hierarchical
            +- cluster search, kmeans, tfidf, countvectorizer, cluster sizes, stopwords lexicon
            += also make a slider for tfidf max features
            += also add the short version
            == also make it so that you can select the certain documents in the cluster and export them/search on them or something
            += also show the words seperately instead of only when hovering
            += also make a cluster wordset into a lexicon

        === results === +
            +- export
            +- delete results; probs some es problem; comes out also in master (not py3 related)
            += also the search result amount is only on the bottom (probably not py3 related)

        === Aggregations === +?
            +-? aggregation; somewhat works, unclear if it works or not since "errors" are also in other branches, also multi aggregation doesnt work (actually just cant aggregate to significant words)
            +- now you can't aggregate to texta_facts, added to reserved (not really py3 related)
            +- also now you cant see the add button when the field#1 aggregation is "significant words". (not really py3 related, javascript change)
            + hide current search in results


    #LEXICON MINER +
        + lm save doesnt work anymore because didnt test after cluster search save
            = local logic error
        +- all word suggestion types
            += actually works, just you cant use it with only one word
            += nevermind still doesn't work for some reason
            += was caused by python2 formats? or zip not throwing an exception
            +- actually randomly clusterings don't work anymore
            +- also term candidates shows raha[punct] now for some reason
            +- also suggest words doesn't work sometimes unless you press reset suggestions
        + save lexicon
            +- also doesn't work for some lexicons, like kohtu
                += was caused by trying to write binary to non binary file
                +=create new lexicon, click on it, find some suggestions, press save, doesn't save
                +=ctrl f5 page, randomly add words there, press save, now saves SOMETIMES (maybe when restarting server works)
        + reset suggestions
        += should display which lexicon is selected.
        += also autoselect newly created lexicon

    #CONCEPTUALISER +
        +- plotting
        + all methods
        + combine
        + dissolve
        + commit results
        == in terminlogy overview when you have no longer existing queries the old concepts still stay
    #MWE MINER +
        + start task works somehow, including different options
        + remove result
        + MWE Task result reverse value list (also got index out of range somewhere, can't replicate)
        + force all true
        + force all false
        += also test with multiple lexicons

    #GRAMMAR MINER +
        + save grammar
        + delete grammar
        + the different grammar options
        + exact match
        + regex
        + test
        + rename
        + unmatched documents
        ?+ highlighter wrong boundaries with regex
            = maybe because when you search words like "Kohtu" you also get "Kohtumäärus" etc, it might use that system to match more.
        +? save forgets options? (nevermind just pressed wrong button)
        +- multiple basics (regex + exact) ajax datatables error, keyerror hits
        ?= when adding new aggregation under concat or gap aggregation, type can only be union and has same slope as original


#################### RESTRICTED+ ###################
    #MODEL MANAGER +
        +- making a model
        + all options

    #CLASSIFICATION MANAGER +
        note: TruncatedSVD removed, replaced with LinearSVC
        ?+ count vectorizer
            + CountVectorizer | TruncatedSVD | Normalizer | LinearSVC
                #- nonneg CountVectorizer | TruncatedSVD | Normalizer | MultinomialNB
            +? prec ill defined CountVectorizer | TruncatedSVD | Normalizer | BernoulliNB
                + more data fixed the problem
            + CountVectorizer | TruncatedSVD | Normalizer | KNeighborsClassifier
            ? prec ill defined CountVectorizer | TruncatedSVD | Normalizer | RadiusNeighborsClassifier
                ? more data just caused the outlier problem - No neighbors found for test samples, you can try
                using larger radius, give a label for outliers, or consider removing them from your dataset.

        +? hashing
            +HashingVectorizer | TruncatedSVD | Normalizer | LinearSVC
                #- non neg HashingVectorizer | TruncatedSVD | Normalizer | MultinomialNB
            +? divide by zero warnings HashingVectorizer | TruncatedSVD | Normalizer | BernoulliNB
                + more data fixed the problem
            + HashingVectorizer | TruncatedSVD | Normalizer | KNeighborsClassifier
            + HashingVectorizer | TruncatedSVD | Normalizer | RadiusNeighborsClassifier

        +? tfidf
            + TfidfVectorizer | TruncatedSVD | Normalizer | LinearSVC
                #- non neg TfidfVectorizer | TruncatedSVD | Normalizer | MultinomialNB
            +? divide by zero warnings TfidfVectorizer | TruncatedSVD | Normalizer | BernoulliNB
                + more data fixed it
            + TfidfVectorizer | TruncatedSVD | Normalizer | KNeighborsClassifier
            + TfidfVectorizer | TruncatedSVD | Normalizer | RadiusNeighborsClassifier

    # ADMINISTRATION +?
        + Superuser false
        + Superuser true
        + Deactivate
        + removing a model
        + Dataset Management
        + Activate
        + Delete
        === script manager ===
            + Create new proj
            + Remove proj
            ? Run script (probably works, just not sure how to test)
    # IMPORTER +?
        + lorem_ipsum.txt imported
        + rutest.txt imported, mlp worked, note /home/eha/Documents/texta/texta-py3/texta/dataset_importer/document_storer/storers/elastic_storer.py line 117, looping over only non mlp values

        types:
            + pdf
            + txt
            + doc
            + docx
            + rtf
            + csv
            + pdf zip
            += json
                works fine for the right format
                removed= now imports different types of json as well, but doesn't show content.

            ?+ xls/xlx fixed in other machine
            - sqlite, https://pastebin.com/faTu10ZE
            + compressed
                +- zip pdf   https://pastebin.com/ExBVcPGz
                +- tar pdf ^
                +- tar gz pdf ^
            =- xml (not implomented error)
            =- html (not implomented error)

        input methods:
            + file upload
            + server_dir
            + url

        +mlp:
            + 1 text doc
        +keep in sync
        + overwrite
        +- not processed fields
#################### OTHER+ #########################


    + alert:
        searcher save, remove, cluster
        bl miner, remove, save
        model manager
        mwe miner; no lexicons
        grammar miner
        concepts saved

        ++ /home/eha/Documents/texta/texta-py3/texta/static/conceptualiser/concept.js
        ++ /home/eha/Documents/texta/texta-py3/texta/static/searcher/searcher.js
        ++ /home/eha/Documents/texta/texta-py3/texta/static/mwe_miner/mwe_miner.js
        ++ /home/eha/Documents/texta/texta-py3/texta/static/model_manager/model_manager.js
        ++ /home/eha/Documents/texta/texta-py3/texta/static/grammar_builder/grammar_builder.js
        ++ /home/eha/Documents/texta/texta-py3/texta/static/classification_manager/classification_manager.js
        ++ /home/eha/Documents/texta/texta-py3/texta/static/base/account.js
        == /home/eha/Documents/texta/texta-py3/texta/static/datatables/datatables.min.js
        == /home/eha/Documents/texta/texta-py3/texta/static/searcher/ColReorderWithResize.js
    === search_api ===
        '{ "auth_token": "3f2aa7b6ec28d4", "searches": [{"dataset": 38}], "aggregation": [
            {"field":"lahendi_aeg","type":"daterange","start":"2000-02-02","end":"2017-09-01","frequency":"raw_frequency","interval":"month"}, {"field": "text.text", "type": "string", "sort_by": "significant_texts"} ] }'
        + get auth token {"auth_token": "3f2aa7b6ec28d4"}
        +-(empty) curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 15 }'
            = unicode is not defined
            = {'error': 'Content-Type header [] is not supported', 'status': 406}
                https://stackoverflow.com/questions/47544966/elasticsearch-content-type-header-application-x-www-form-urlencoded-is-not-s
                curl -H "Content-Type: application/json" -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 15 }
            + fixed by adding headers={"Content-Type": "application/json"} to request in searcher:100
        +-(field) curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 15 "fields": ["text", "texta_facts"], "parameters": {"limit": 100} }'
            = fields disppeared, now is _source, rework in docu
        + (string query) curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 15, "fields": ["text"], "parameters": {"limit": 15}, "constraints": [ {"field":"text","operator":"must","type":"match_phrase","slop":0,"strings":["Laura Sirelpuu"], "class":"string"} ] }'
            curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 38, "fields": ["text.text"], "parameters": {"limit": 15}, "constraints": [ {"field":"text.text","operator":"must","type":"match_phrase","slop":0,"strings":["Laura Sirelpuu"], "class":"string"} ] }'
        +?(date) curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 15, "fields": ["text"], "parameters": {"limit": 15}, "constraints": [ {"field":"text","operator":"must","type":"match_phrase","slop":0,"strings":["Laura Sirelpuu"], "class":"string"}, {"field":"published", "class":"date", "start":"2017-01-01", "end":"2017-12-31"} ] }'
            curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 38, "fields": ["text.text"], "parameters": {"limit": 15}, "constraints": [ {"field":"text.text", "class":"date", "start":"2000-01-01", "end":"2007-12-31"}, {"field":"text.text","operator":"must","type":"match_phrase","slop":0,"strings":["Laura Sirelpuu"], "class":"string"}  ] }'
        +? (scroll) curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 15, "fields": ["text"], "parameters": {"limit": 15}, "constraints": [ {"field":"text","operator":"must","type":"match_phrase","slop":0,"strings":["madis kägu"], "class":"string"} ], "scroll":true}' -o 'Documents/test.json'
            ?curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 15, "fields": ["text"], "parameters": {"limit": 15}, "constraints": [ {"field":"text","operator":"must","strings":["Laura Sirelpuu"], "class":"fact"} ] }'
            retunrs {"text": } values but not scroll id, total etc?
        +operators:
            +must
            +should
            +must_not
        +type:
            +match
            +match_phrase
            +match_phrase_prefix
        += temporal/date
        + {"field":"content","operator":"must","strings":["president", "prime_minister"], "class":"fact"}
            + curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 34, "fields": ["mlp_text.text"], "parameters": {"limit": 15}, "constraints": [ {"field":"mlp_text.text","operator":"should","strings":["PER"], "class":"fact"} ] }'
            
        + fact_val search
            + curl -XPOST 'http://localhost:8000/api/search' -d '{ "auth_token": "3f2aa7b6ec28d4", "dataset": 34, "fields": ["mlp_text.text"], "parameters": {"limit": 15}, "constraints": [ {"field":"mlp_text.text","operator":"must","type":"str","constraints":[{"name":"LOC", "operator":"=", "value": "Eesti"}], "class":"fact_val"} ] }'
        +? aggregation also has significant_texts now, maybe need to update documentatation
        +advanced fact_str, fact_num
        +?list datasets search_api should be api in url in documentatation
        +? auth_token shouldnt end with a comma in documentatation, throws error
        + list dataset, documentation url is wrong
        + importing

    === remove junk files ===
        + texta/files/dataset_importer/*
        + texta/files/database/*
        + migrations
        + files marked in gitignore

        april 23:
            + Local directory to Host directory
            + Fixed search hl spans
            + Fixed some alerts (prompts, confirms)
            + Dataset importer view details css fix
            + Json bugfix
